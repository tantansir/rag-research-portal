# Personal Research Portal (PRP)

A research-grade RAG portal for the **City Digital Twins** domain. The system ingests a domain corpus, retrieves evidence with hybrid search, produces citation-backed answers, and generates exportable research artifacts.

**Domain**: City Digital Twins, Urban AI, and Spatial Intelligence  
**Main Research Question**: How do city digital twins enable evidence-backed decision support for urban planning and management?

---

## Quick Start

### Prerequisites
- Python 3.10 or 3.11  
- A [Google Gemini API key](https://aistudio.google.com/app/apikey)

### 1 Â· Install dependencies
```bash
pip install -r requirements.txt
```

### 2 Â· Set environment variables
```powershell
# Windows PowerShell
$env:GEMINI_API_KEY    = "your_key_here"      
$env:GEMINI_MODEL      = "gemini-2.5-flash"   # You can switch to other models such as gemini-2.5-pro, gemini-3-pro-preview
$env:GEMINI_EVAL_MODEL = "gemini-2.5-flash"
```
```bash
# macOS / Linux
export GEMINI_API_KEY="your_key_here"
export GEMINI_MODEL="gemini-2.5-flash"
export GEMINI_EVAL_MODEL="gemini-2.5-flash"
```

### 3 Â· Launch the web portal
```bash
python run_app.py
# or
streamlit run src/app/app.py
```

Opens at **http://localhost:8501**.

---

## Repository Structure

```
phase2-prp/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Downloaded PDFs (22 sources)
â”‚   â”œâ”€â”€ processed/                 # Extracted text + chunk JSON per source
â”‚   â”œâ”€â”€ embeddings/                # FAISS index, BM25 index, embeddings cache
â”‚   â””â”€â”€ data_manifest.csv          # Source metadata (22 sources, all required fields)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest/
â”‚   â”‚   â”œâ”€â”€ collect_corpus.py      # arXiv search + download; AuthorYear source_id generation
â”‚   â”‚   â””â”€â”€ ingest_pipeline.py     # PDF parsing, cleaning, section-aware chunking
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â””â”€â”€ rag_system.py          # Hybrid RRF retrieval + LLM reranking + citation generation
â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â”œâ”€â”€ evaluator.py           # Groundedness / citation precision / answer relevance
â”‚   â”‚   â””â”€â”€ query_set.json         # 22 fixed evaluation queries
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ app.py                 # Streamlit portal (Phase 3)
â”‚       â”œâ”€â”€ thread_manager.py      # Save / search / delete research threads
â”‚       â”œâ”€â”€ artifact_generator.py  # Evidence table, bibliography, synthesis memo, disagreement map
â”‚       â””â”€â”€ export_manager.py      # Markdown / CSV / PDF / BibTeX export
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ eval/                      # Evaluation CSVs, JSONL run logs, Markdown reports
â”‚   â”œâ”€â”€ threads/                   # Saved research threads (JSON)
â”‚   â””â”€â”€ exports/                   # Generated and exported research artifacts
â”œâ”€â”€ logs/                          # Per-day JSONL query logs
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ Phase1_Framing_Report.pdf
â”‚   â””â”€â”€ Phase2_Evaluation_Report.md
â”œâ”€â”€ run_app.py                     # Launch script
â”œâ”€â”€ requirements.txt               # Pinned dependencies
â”œâ”€â”€ AI_USAGE_LOG.md
â””â”€â”€ README.md
```

---

## Phase 3 â€” Research Portal (MVP + Stretch Goals)

### Core MVP Features

| Feature | Location in UI |
|---------|----------------|
| **Search & Ask** â€” hybrid retrieval, citation-backed answers | Navigation â†’ Search & Ask |
| **Metadata filters** â€” year range + source type | Search & Ask â†’ Filter panel |
| **Research Threads** â€” save / search / delete sessions | Navigation â†’ Research Threads |
| **Evidence Table** â€” Claim Â· Evidence Â· Citation Â· Confidence Â· Notes | Artifacts â†’ Evidence Table |
| **Annotated Bibliography** â€” 8â€“12 sources with 4 fields | Artifacts â†’ Annotated Bibliography |
| **Synthesis Memo** â€” 800â€“1200 words with inline citations | Artifacts â†’ Synthesis Memo |
| **Export** â€” Markdown / CSV / PDF | All artifact types |
| **Evaluation View** â€” run query set, metrics, baseline vs enhanced | Navigation â†’ Evaluation |

### Stretch Goals Implemented

| Stretch Goal | Spec Reference | Location in UI |
|---|---|---|
| ğŸ”­ **Gap Finder** | "Gap finder: missing evidence + targeted next retrieval actions" | Navigation â†’ Gap Finder |
| âš¡ **Disagreement Map** | "Automatic disagreement map (conflicts surfaced with citations)" | Artifacts â†’ Disagreement Map |
| ğŸ“„ **BibTeX Export** | "BibTeX export" | Artifacts â†’ Annotated Bibliography â†’ Export BibTeX |
| ğŸ”§ **Metadata Filters** | "Filters by year/venue/type" | Search & Ask â†’ Filter panel |

### Artifact Schemas

**Evidence Table** (CSV / PDF):  
`Claim | Evidence Snippet | Citation (source_id, chunk_id) | Confidence | Notes`

**Annotated Bibliography** (CSV / BibTeX):  
`Source ID | Title | Authors | Year | Venue | DOI/URL | Claim | Method | Limitations | Why it matters | Chunks Retrieved`

**Synthesis Memo** (Markdown / PDF):  
800â€“1200 words with inline `(source_id, chunk_id)` citations and a full reference list generated from the data manifest.

**Disagreement Map** (CSV):  
`Aspect | Source A | Position A | Source B | Position B | Conflict Type`

### Trust Behavior

- Every answer cites only retrieved chunks using `(source_id, chunk_id)`.
- A post-generation validation pass detects invalid citations; a repair pass corrects them.
- If evidence is absent the system explicitly states this rather than hallucinating.
- Filter zero-hit: if year/type filters eliminate all results, the app shows a warning instead of returning silently degraded output.

---

## Phase 2 â€” RAG System

### Corpus

| Property | Value |
|----------|-------|
| Total sources | 22 |
| Peer-reviewed journal articles | 11 |
| arXiv preprints | 10 |
| Technical reports | 1 |
| Year range | 2021â€“2025 |

All sources have: `source_id Â· title Â· authors Â· year Â· source_type Â· venue Â· url_or_doi Â· raw_path Â· processed_path Â· tags Â· relevance_note`

Source IDs follow the **AuthorYear** convention (e.g. `Luo2024`, `WEF2022`). Every citation maps back to a row in `data/data_manifest.csv`.

### Retrieval Pipeline

```
Query
  â”‚
  â”œâ”€ Vector search  (FAISS, all-MiniLM-L6-v2)  â”€â”
  â”‚                                               â”œâ”€ RRF fusion â†’ top-k candidates
  â””â”€ Lexical search (BM25Okapi)                  â”€â”˜
                â”‚
                â–¼
         Reference-section filter
                â”‚
                â–¼
         LLM reranker (Gemini, 400-char previews)
                â”‚
                â–¼
         Answer generation with strict citation constraint
                â”‚
                â–¼
         Citation validation + repair pass
```

**Enhancement**: Hybrid retrieval via **Reciprocal Rank Fusion (RRF)** + **LLM reranking**.  
RRF is robust to score-scale mismatch between BM25 and cosine similarity, which is why it outperforms the linear-score fusion used in the baseline.

### Evaluation

**Query set**: 22 queries â€” 11 direct, 6 synthesis/multi-hop, 5 edge-case / ambiguity  
**Metrics**: Groundedness (LLM judge 1â€“4) Â· Citation precision (exact match %) Â· Answer relevance (LLM judge 1â€“4)

Latest comparison results (see `outputs/eval/`):

| Metric | Baseline | Enhanced | Delta |
|---|---:|---:|---:|
| Groundedness (avg /4) | 2.91 | 3.91 | 1.00 |
| Citation precision (avg) | 99.43% | 99.65% | 0.22% |
| Answer relevance (avg /4) | 3.41 | 3.55 | 0.14 |

### Logging & Reproducibility

- Every query writes a JSONL record to `logs/query_log_YYYYMMDD.jsonl` containing: `run_id Â· query Â· prompt_version Â· prompt_hash Â· model Â· retrieval_config Â· retrieved_chunks Â· answer Â· citations_used`
- All dependencies are pinned in `requirements.txt`
- Embedding cache is validated against a SHA-1 of `all_chunks.json`; stale cache auto-rebuilds

---

## CLI Usage (Phase 2)

```bash
# Single query â€” vector-only baseline
python -m src.rag.rag_system --query "What is an urban digital twin?" --show_evidence

# Single query â€” enhanced (hybrid RRF + reranking)
python -m src.rag.rag_system \
  --query "What is an urban digital twin?" \
  --use_hybrid --use_reranking --top_k_after_rerank 5 \
  --show_evidence

# Run full evaluation (22 queries, enhanced)
python -m src.eval.evaluator

# Run baseline vs enhanced comparison
python -m src.eval.evaluator --compare

# Run comparison on a subset (faster, e.g. first 5 queries)
python -m src.eval.evaluator --compare --ids Q01,Q02,Q03,Q04,Q05
```

## Rebuilding the Corpus from Scratch

```bash
# Step 1: Download arXiv papers (optional â€” PDFs already in data/raw/)
python -m src.ingest.collect_corpus

# Step 2: Parse PDFs, chunk text, build all_chunks.json
python -m src.ingest.ingest_pipeline

# Step 3: Rebuild FAISS + BM25 index
#   (happens automatically on next app or rag_system startup)
```

---

## Citation Format

All answers use `(source_id, chunk_id)` inline citations.

Example: `(Luo2024, chunk_03)` resolves to:
- **Manifest row**: `data/data_manifest.csv` â†’ row with `source_id = Luo2024`
- **Chunk text**: `data/processed/Luo2024_chunks.json` â†’ item with `chunk_id = chunk_03`
- **Raw source**: `data/raw/Luo2024.pdf`

---

## Phase 1 Report

See `report/Phase1_Framing_Report.pdf` for the research framing, prompt kit (paper triage + claim-evidence extraction tasks), evaluation sheet (16 runs across 2 models Ã— 2 prompts Ã— 2 test cases), and analysis memo.