# AI Usage Log

This log documents all AI tools used in building this project, what each tool contributed, and what was manually verified or changed afterward — as required by the project specification.

---

## Tool 1: Google Gemini API (Runtime)

**Purpose**: Answer generation, LLM reranking, LLM-based evaluation scoring, Gap Finder analysis, Disagreement Map generation.

**How it is used in code**:
- `src/rag/rag_system.py` — `generate_answer()`, `rerank_with_llm()`, citation repair pass
- `src/eval/evaluator.py` — `evaluate_groundedness()`, `evaluate_answer_relevance()`
- `src/app/artifact_generator.py` — `find_research_gaps()`, `generate_disagreement_map()`

**Manual changes after AI output**:
- Added post-generation citation validation (`_validate_citations`) and a repair pass (`_repair_answer_with_allowed_citations`) to enforce strict `(source_id, chunk_id)` format.
- Tuned the strict-citation prompt to require one citation per parentheses and per-claim coverage.
- Switched evaluation judge to `gemini-2.5-flash` (faster, sufficient for judge tasks) and added context trimming (`_trim_chunks_for_eval_context`) to reduce timeout rate.
- Confirmed all gap and disagreement outputs manually against retrieved evidence.

---

## Tool 2: Claude (Anthropic) — Development Assistant

**Purpose**: Code generation and iterative debugging for the full Phase 2 and Phase 3 implementation.

**Specific contributions**:

| Area | What Claude generated | What was manually verified / changed |
|------|-----------------------|--------------------------------------|
| `rag_system.py` — hybrid retrieval | Rewrote `hybrid_retrieve()` to use Reciprocal Rank Fusion (RRF) instead of linear score combination | Verified RRF formula against original paper; confirmed retrieval quality improvement in evaluation results |
| `rag_system.py` — metadata filter | Added `_get_allowed_source_ids()` and filter logic in `query()` | Tested year-range and source-type filters manually; added zero-hit warning path |
| `evaluator.py` — speed fix | Rewrote judge prompts to use trimmed context; changed default eval model to flash | Ran timed comparison to confirm latency reduction |
| `evaluator.py` — comparison | Added `run_enhancement_comparison()` with baseline vs enhanced labelling | Verified that baseline and enhanced kwargs reach different code paths via `retrieval_config` log |
| `app.py` — Gap Finder page | Full `show_gap_finder_page()` function | Tested gap detection on 3 different queries; confirmed suggested follow-up queries are sensible |
| `app.py` — Disagreement Map | `generate_disagreement_map()` + UI rendering with expanders | Verified JSON parsing robustness on malformed LLM outputs |
| `app.py` — BibTeX export | `export_bibtex()` in `export_manager.py` | Validated output `.bib` files in Zotero; fixed entry-type detection for arXiv papers |
| `app.py` — widget key fix | Suggested `chunk_{source_id}_{chunk_id}` key pattern | Fixed `DuplicateWidgetID` error that appeared when multiple sources returned same chunk_id |
| `collect_corpus.py` | `make_source_id()` function generating AuthorYear IDs | Manually curated the 23-source ID mapping; ran `rename_source_ids.py` migration script |
| `rename_source_ids.py` | Migration script renaming all source IDs across data files | Ran the script, inspected diff on `all_chunks.json`, `data_manifest.csv`, and log files |
| `README.md` | Full README draft | Verified all commands run correctly on local machine; corrected folder structure and metric values from actual eval results |

**What was not generated by AI**:
- The 22-source corpus selection and PDF acquisition were done manually.
- The `data/data_manifest.csv` metadata (authors, venue, DOI, relevance notes) was manually verified against each source.
- The Phase 1 prompt kit, evaluation sheet, and framing brief were written manually.
- All evaluation runs were executed locally and results reviewed for plausibility.
---

## Tool 3: ChatGPT (OpenAI) — Early Scaffolding

**Purpose**: Drafted initial pipeline structure during Phase 2 prototyping (ingestion flow, chunking strategy, evaluation rubric outline).

**Manual changes after AI output**:
- Rewrote the ingestion pipeline to use `pdfplumber` instead of the suggested `PyPDF2` (better table and column handling for academic PDFs).
- Replaced the suggested fixed-size chunking with section-aware chunking (`detect_sections()` in `ingest_pipeline.py`).
- Discarded the suggested cosine-similarity-only retrieval in favour of the hybrid RRF approach implemented in Phase 3.

---

## Summary

All AI-generated code and text was reviewed, tested end-to-end, and corrected where needed. AI tools were used to accelerate implementation; all research decisions (corpus selection, evaluation design, metric interpretation) were made by the student.
